Hadoop 完全分布式安装-Hadoop—2.7.4

1。获取安装包

hadoop.apache.com

2. 配置环境Java环境变量

 2.1 获取jdk安装包
 	java.com

 2.2 解压jdk
  $ sudo tar -zxvf jdk1.8.0_144.tar.gz -C /usr/local/
 2.3 配置用户环境变量
  //创建hadoop用户
  $ sudo useradd hadoop
  $ sudo passwd hadoop --密码为hadoop123
  $ su - hadoop
  $ vim ~/.bashrc
 	#++++++++++++++++++++++++++++++++++++++++
	export JAVA_HOME=/usr/local/jdk1.8.0_144/
	export JDK_BASH=//usr/local/jdk1.8.0_144/jre
	export CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JDK_BASH/lib
	export PATH=$PATH:$JAVA_HOME/bin:$JDK_BASH/bin
 $ source ~/.bashrc
 $ java -version
3.配置Hadoop-2.7.4分布式安装
 3.1 解压Hadoop-2.7.4 安装包
  $ tar -zxvf hadoop-2.7.4.tar.gz -C /usr/local/
 3.2 配置Hadoop变量
  $ vim ~/.bashrc
	export HADOOP_HOME=/usr/local/hadoop-2.7.4
	export HADOOP_INSTALL=$HADOOP_HOME
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
  $ source ~/.bashrc
  $ hadoop version
 3.3 配置Hadoop配置文件
 	<!--所有配置文件里的IP为master地址，也可以使用域名-->
  3.3.1 配置hdfs的配置文件
  $ cd /usr/local/hadoop-2.7.4/etc/hadoop
  $ vim hdfs-site.xml
  <configuration>

	<property>
	    <name>dfs.permissions.enabled</name>
	    <value>false</value>
	  </property>
	  <!--开启dfs的web服务-->
	  <property>
	    <name>dfs.webhdfs.enabled</name>
	    <value>true</value>
	  </property>
	  <property>
	    <name>dfs.blocksize</name>
	    <value>64m</value>
	  </property>
	  <property>
	    <name>dfs.nameservices</name>
	    <value>hadoop-cluster-zgw</value>
	  </property>
	  <!-- 创建几个副本集-->
	  <property>
	    <name>dfs.replication</name>
	    <value>2</value>
	  </property>
	  <!--创建namenode的存储路径-->
	  <property>
	    <name>dfs.namenode.name.dir</name>
	    <value>file:/usr/local/hadoop-2.7.4/hdfs/name</value>
	  </property>
	  <!--创建checkpoint的存储路径-->
	  <property>
	    <name>dfs.namenode.checkpoint.dir</name>
	    <value>file:/usr/local/hadoop-2.7.4/hdfs/checkpoint</value>
	  </property>
	  <!--创建checkpoint的存储路径-->
	  <property>
	    <name>dfs.namenode.checkpoint.edits.dir</name>
	    <value>file:/usr/local/hadoop-2.7.4/hdfs/checkpoint</value>
	  </property>
	  <!--创建数据存储的路径-->
	  <property>
	    <name>dfs.datanode.data.dir</name>
	    <value>file:/usr/local/hadoop-2.7.4/hdfs/data</value>
	  </property>
	  <!--创建主服务启动的地址和端口-->
	  <property>
	    <name>dfs.namenode.secondary.http-address</name>
	    <value>192.168.240.133:50090</value>
	  </property>
	</configuration>
  3.3.2 配置 yarn的配置文件
  $ vim yarn-site.xml
  <configuration>

	<!-- Site specific YARN configuration properties -->

	<property>
	    <name>yarn.nodemanager.aux-services</name>
	    <value>mapreduce_shuffle</value>
	  </property>
	  <property>
	    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
	    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	  </property>
	  <property>
	    <name>yarn.nodemanager.local-dirs</name>
	    <value>/usr/local/hadoop-2.7.4/yarn/nodemanager</value>
	  </property>
	  <property>
	    <name>yarn.resourcemanager.hostname</name>
	    <value>192.168.240.133</value>
	  </property>
	  <property>
	    <name>yarn.resourcemanager.address</name>
	    <value>192.168.240.133:8032</value>
	  </property>
	  <property>
	    <name>yarn.resourcemanager.scheduler.address</name>
	    <value>192.168.240.133:8030</value>
	  </property>
	  <property>
	    <name>yarn.resourcemanager.resource-tracker.address</name>
	    <value>192.168.240.133:8031</value>
	  </property>
	  <property>
	    <name>yarn.resourcemanager.admin.address</name>
	    <value>192.168.240.133:8033</value>
	  </property>
	  <property>
	    <name>yarn.resourcemanager.webapp.address</name>
	    <value>192.168.240.133:8034</value>
	  </property>

	</configuration>

  3.3.3 配置core
  $ vim core-site.xml
   <configuration>
	<property>
	       <name>fs.defaultFS</name>
	        <value>hdfs://192.168.240.133:9000</value>
	</property>

	 <property>
	    <name>io.file.buffer.size</name>
	    <value>131072</value>
	  </property>

	<property>
	        <name>hadoop.tmp.dir</name>
	        <value>file:/usr/local/hadoop-2.7.4/hdfs/tmp</value>
	</property>
	</configuration>
  3.3.4 配置mapred
   $ vim mapred-site.xml
   <configuration>

	 <property>
	    <name>mapreduce.framework.name</name>
	    <value>yarn</value>
	  </property>
	  <property>
	    <name>mapreduce.jobhistory.address</name>
	    <value>192.168.240.133:10020</value>
	  </property>
	  <property>
	    <name>mapreduce.jobhistory.webapp.address</name>
	    <value>192.68.240.133:19888</value>
	  </property>
	</configuration>
  3.3.5 配置slave
  $ vim slaves
  bigdata2
  bigdata3
4.配置hosts解析和ssh无密码登录
 4.1 配置hosts
 <!--所有节点都要配置-->
  $ vim /etc/hosts
  192.168.240.133	bigdata1
  192.168.240.134	bigdata2
  192.168.240.135	bigdata3
 4.2 配置ssh无密码登录
  4.2.1 查看当前用户
  $ whoami
  hadoop
  <!--所有节点都要配置-->
  4.2.2 创建ssh的key
  $ ssh-keygen -t rsa
  $ ssh-copy-id -i bigdata1
  $ ssh-copy-id -i bigdata2
  $ ssh-copy-id -i bigdata3
  4.2.3 测试ssh无密码登录
  $ ssh bigdata2
5.创建配置文件里的目录
  <!--所有节点都要创建-->
  $ sudo mkdir -p /usr/local/hadoop-2.7.4/hdfs
  $ sudo mkdir -p /usr/local/hadoop-2.7.4/yarn
  $ sudo mkdir -p /usr/local/hadoop-2.7.4/tmp
6.拷贝master 节点的Hadoop到其它节点
  $ scp -R hadoop-2.7.4/ bigdata2:/usr/local/
  $ scp -R hadoop-2.7.4/ bigdata3:/usr/local/
7.修改权限
  <!--所有节点都要配置-->
  $sudo chown -R hadoop.hadoop /usr/local/hadoop-2.7.4
8.启动Hadoop集群
  $ start-all.sh
  <!--观察日志，查看是否有错误 ，有错误，根据错误处理-->